{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to use this script\n",
    "\n",
    "/!\\ To run this script, you need a working version of python with **numpy** and **panda** libraries\n",
    "\n",
    "1. Compile the cell of **import libraries**\n",
    "2. Fill the **user defined** part as follows, and compile it :\n",
    "    - For step1_in and step2_in :\n",
    "        * put the names of the different countries in the list countries\n",
    "        * put the path to corresponding data files (those files must follow the template file -> see...)\n",
    "        * set the number of timeseries which have a WEIGHT defined in N_ts variable\n",
    "        * set the number of TD to compute\n",
    "    - For step2_in only : \n",
    "        * set the peah_sh_factor\n",
    "        * put the path of the output of STEP_1 : TD_of_days.out\n",
    "3. **Compile the functions**\n",
    "4. **Call the functions**\n",
    "    - REM : \n",
    "        * before to call the functions, one must close all the DATA.xls\n",
    "        * before calling the function step2_in, one must run the clustering method of STEP_1\n",
    "        * for clustering method don't forget to update the set DIMENSIONS in the TD_main.mod and to delete the TD_of_days.out file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Recipe to add a timeseries :\n",
    "> * add the times series columns into the DATA.xlsx in sheet '1.1 Time Series' (also better to update the sheet 1.2 c_p_t but it will work without it)\n",
    "> * if this timeseries should be used for TD clustering,\n",
    ">   * add a line to the weights in sheet '2.2 User defined' with the exact same name as the column.\n",
    ">   * change N_ts value in this script\n",
    ">   * add the names of the ts and its coresponding parameter either to EUD_params, or RES_params, or RES_mult_params according to which category it corresponds\n",
    "> * Run the script according to the explanations here above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) User Defined\n",
    "## Data for reading files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for step1_in and step2_in ##\n",
    "countries = ['BE', 'CH', 'FR'] # countries list\n",
    "data = ['Data/DATA_BE.xlsx', 'Data/DATA_CH.xlsx', 'Data/DATA_FR.xlsx'] # data path (in same order as data path)\n",
    "N_ts = 7 # number of timeseries with a WEIGHT defined (per country)\n",
    "Nbr_TD = 12 # number of typical day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for step2_in ##\n",
    "#peak_sh_factor = {'BE' : 1.33, 'CH' : 1.10, 'FR' : 1.18} # should be computed as max(SH_TS)/max(SH_TS_TD)\n",
    "step1_out = 'STEP_1_TD_selection/TD_of_days.out'\n",
    "## name of timeseries in DATA.xlsx and corresponding name in ESTD data file\n",
    "# for EUD timeseries\n",
    "EUD_params = {'Electricity (%_elec)' : 'param electricity_time_series :=', 'Space Heating (%_sh)' : 'param heating_time_series :=', 'Space Cooling':'param cooling_time_series :=', 'Passanger mobility (%_pass)' : 'param mob_pass_time_series :=', 'Freight mobility (%_freight)' : 'param mob_freight_time_series :='}\n",
    "# for resources timeseries that have only 1 tech linked to it\n",
    "RES_params = {'PV' : 'PV', 'Wind_offshore' : 'WIND_OFFSHORE', 'Wind_onshore': 'WIND_ONSHORE'}\n",
    "# for resources timeseries that have several techs linked to it\n",
    "RES_mult_params = {'Tidal' : ['TIDAL_STREAM', 'TIDAL_RANGE'], 'Hydro_dam' : ['HYDRO_DAM'], 'Hydro_river' : ['HYDRO_RIVER'], 'Solar' : ['DHN_SOLAR', 'DEC_SOLAR', 'PT_COLLECTOR', 'ST_COLLECTOR', 'STIRLING_DISH']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Compiling the functions\n",
    "## From DATA to STEP_1_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step1_in(countries,data, N_ts=6, Nbr_TD=12):\n",
    "    \"step1_in reads the datas at the path of data and prints the Ndata param in Ndata.tsv and the list of timeseries used in this Ndata with their weights and norm\"\n",
    "    N_c = len(data) #number of countries\n",
    "    \n",
    "    ## READ FIRST COUNTRY ##\n",
    "    #reading the weights\n",
    "    weights = pd.read_excel(data[0],  sheet_name='2.2 User defined',  header=[4], index_col=0, nrows = N_ts, usecols = [0,1] ).rename(columns={'Unnamed: 1':'Weights'})\n",
    "    weights.index.rename('Category', inplace=True)\n",
    "    # from weights, create the names of ts for STEP1 and number of ts\n",
    "    ts_names = list(weights.index)\n",
    "\n",
    "    #reading the timeseries\n",
    "    timeseries =  pd.read_excel(data[0],  sheet_name='1.1 Time Series',  header=[1], index_col=0, nrows = 8760 )\n",
    "    timeseries.drop(labels='period_duration [h]', axis=1, inplace=True)\n",
    "\n",
    "    # adding the country suffix\n",
    "    ts_names_all = [x+'_'+countries[0] for x in ts_names]\n",
    "    weights.index =  [str(line) + '_' + countries[0] for line in weights.index]\n",
    "    timeseries.columns = [str(col) + '_' + countries[0] for col in timeseries.columns]\n",
    "    \n",
    "    ## READING THE OTHER COUNTRIES ##\n",
    "    for i in np.arange(1,N_c):\n",
    "        #reading the weights\n",
    "        weights2 = pd.read_excel(data[i],  sheet_name='2.2 User defined',  header=[4], index_col=0, nrows = N_ts, usecols = [0,1] ).rename(columns={'Unnamed: 1':'Weights'})\n",
    "        weights2.index =  [str(line) + '_' + countries[i] for line in weights2.index]\n",
    "        weights = pd.concat([weights, weights2]) #weights = weights.merge(pd.read_excel(data[i],  sheet_name='2.2 User defined',  header=[4], index_col=0, nrows = N_ts, usecols = [0,1] ).rename(columns={'Unnamed: 1':'Weights_'+countries[i]}), left_index=True, right_index=True)\n",
    "        # adding the names of each country's columns that have a weight\n",
    "        ts_names_all = ts_names_all + [x+'_'+countries[i] for x in ts_names]\n",
    "        #reading the timeseries\n",
    "        ts2 = pd.read_excel(data[i],  sheet_name='1.1 Time Series',  header=[1], index_col=0, nrows = 8760).drop(labels='period_duration [h]', axis=1)\n",
    "        ts2.columns = [str(col) + '_' + countries[i] for col in ts2.columns]\n",
    "        timeseries = timeseries.merge(ts2, left_index=True, right_index=True)\n",
    "    \n",
    "    ## NORMALIZING TIMESERIES ##\n",
    "    #compute norm = sum(ts)\n",
    "    norm = timeseries.sum(axis=0)\n",
    "    norm.index.rename('Category', inplace=True)\n",
    "    norm.name = 'Norm'\n",
    "    # normalise ts to have sum(norm_ts)=1\n",
    "    norm_ts = timeseries/norm\n",
    "    # fill NaN with 0\n",
    "    norm_ts.fillna(0, inplace=True)\n",
    "    \n",
    "    ## WEIGHTING TIMESERIES ##\n",
    "    weights = pd.Series(data=weights['Weights'], index=weights.index)\n",
    "    weights.index.rename('Category', inplace=True)\n",
    "    # select columns of ts that matters for STEP1\n",
    "    weight_ts = norm_ts[ts_names_all]\n",
    "    # multiply each timeserie by its weight\n",
    "    weight_ts = weight_ts*weights\n",
    "    \n",
    "    ## CREATING DAY AND HOUR COLUMNS (for later pivoting) ##\n",
    "    # creating df with 2 columns : day of the year | hour in the day\n",
    "    day_and_hour_array = np.ones((24*365,2))\n",
    "    for i in range(365):\n",
    "        day_and_hour_array[i*24:(i+1)*24,0] = day_and_hour_array[i*24:(i+1)*24,0]*(i+1)\n",
    "        day_and_hour_array[i*24:(i+1)*24,1] = np.arange(1,25,1)\n",
    "    day_and_hour = pd.DataFrame(day_and_hour_array, index = np.arange(1,8761,1), columns=['D_of_H','H_of_D'])\n",
    "    day_and_hour = day_and_hour.astype('int64')\n",
    "    # merge day_and_hour with weight_ts for later pivot\n",
    "    weight_ts = weight_ts.merge(day_and_hour,left_index=True, right_index=True)\n",
    "    \n",
    "    ## CREATING NDATA ##\n",
    "    # pivoting timeseries to get Ndata (but not normalised and weighted)\n",
    "    Ndata = weight_ts.pivot(index='D_of_H', columns='H_of_D', values=ts_names_all)\n",
    "    # renumeroting Ndata columns\n",
    "    Ndata.columns = np.arange(1,24*N_ts*N_c+1)\n",
    "    # adding AMPL syntax for param Ndata\n",
    "    Ndata.rename(columns = {Ndata.shape[1]:str(Ndata.shape[1])+' '+':='}, inplace=True)\n",
    "    \n",
    "    ## PRINTING NDATA AND WEIGHTS AND NORMS ##\n",
    "    out_path = 'STEP_1_TD_selection/data.dat'\n",
    "    \n",
    "    with open(out_path, mode='w',newline='') as TD_file:\n",
    "        TD_writer = csv.writer(TD_file, delimiter='\\t', quotechar=' ', quoting=csv.QUOTE_MINIMAL)\n",
    "\n",
    "        TD_writer.writerow(['# -------------------------------------------------------------------------------------------------------------------------\t'])\n",
    "        TD_writer.writerow(['#\tEnergyScope TD is an open-source energy model suitable for country scale analysis. It is a simplified representation of an urban or national energy system accounting for the energy flows'])\n",
    "        TD_writer.writerow(['#\twithin its boundaries. Based on a hourly resolution, it optimises the design and operation of the energy system while minimizing the cost of the system.'])\n",
    "        TD_writer.writerow(['#\t'])\n",
    "        TD_writer.writerow(['#\tCopyright (C) <2018-2019> <Ecole Polytechnique Fédérale de Lausanne (EPFL), Switzerland and Université catholique de Louvain (UCLouvain), Belgium>'])\n",
    "        TD_writer.writerow(['#\t'])\n",
    "        TD_writer.writerow(['#\t'])\n",
    "        TD_writer.writerow(['#\tLicensed under the Apache License, Version 2.0 (the \"License\");'])\n",
    "        TD_writer.writerow(['#\tyou may not use this file except in compliance with the License.'])\n",
    "        TD_writer.writerow(['#\tYou may obtain a copy of the License at'])\n",
    "        TD_writer.writerow(['#\t'])\n",
    "        TD_writer.writerow(['#\thttp://www.apache.org/licenses/LICENSE-2.0'])\n",
    "        TD_writer.writerow(['#\t'])\n",
    "        TD_writer.writerow(['#\tUnless required by applicable law or agreed to in writing, software'])\n",
    "        TD_writer.writerow(['#\tdistributed under the License is distributed on an \"AS IS\" BASIS,'])\n",
    "        TD_writer.writerow(['#\tWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.'])\n",
    "        TD_writer.writerow(['#\tSee the License for the specific language governing permissions and'])\n",
    "        TD_writer.writerow(['#\tlimitations under the License.'])\n",
    "        TD_writer.writerow(['#\t'])\n",
    "        TD_writer.writerow(['#\tDescription and complete License: see LICENSE file.'])\n",
    "        TD_writer.writerow(['# -------------------------------------------------------------------------------------------------------------------------\t'])\n",
    "        TD_writer.writerow(['\t'])\n",
    "        TD_writer.writerow(['# SETS depending on TD\t'])\n",
    "        TD_writer.writerow(['# --------------------------\t'])\n",
    "        TD_writer.writerow(['param Nbr_TD :=\t'+str(Nbr_TD)])\n",
    "        TD_writer.writerow([';\t\t'])\n",
    "        TD_writer.writerow(['\t\t'])\n",
    "        \n",
    "    # concatenating and printing weights and norm\n",
    "    weight_norm = pd.concat([weights, norm[ts_names_all]], axis=1)\n",
    "    weight_norm.reset_index(inplace=True)\n",
    "    weight_norm['#'] = '#'\n",
    "    weight_norm = weight_norm[['#','Category', 'Weights', 'Norm']]\n",
    "    weight_norm.to_csv(out_path, sep='\\t', header=True, index=False, mode='a')\n",
    "    \n",
    "    with open(out_path, mode='a',newline='') as TD_file:\n",
    "        TD_writer = csv.writer(TD_file, delimiter='\\t', quotechar=' ', quoting=csv.QUOTE_MINIMAL)\n",
    "        TD_writer.writerow([''])\n",
    "    \n",
    "    # printing param Ndata\n",
    "    Ndata.to_csv(out_path, sep='\\t', header=True, index=True, index_label='param Ndata :', mode='a')\n",
    "    \n",
    "    with open(out_path, mode='a',newline='') as TD_file:\n",
    "        TD_writer = csv.writer(TD_file, delimiter='\\t', quotechar=' ', quoting=csv.QUOTE_MINIMAL)\n",
    "        TD_writer.writerow([';'])\n",
    "    \n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From STEP_1_out to STEP_2_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step2_in(countries, data, step1_out, EUD_params, RES_params, RES_mult_params, N_ts=6, Nbr_TD=12):\n",
    "    ## READING OUTPUT OF STEP1 ##\n",
    "    TD_of_days = pd.read_csv(step1_out, names=['TD_of_days'])\n",
    "    TD_of_days['day'] = np.arange(1,366,1) # putting the days of the year beside\n",
    "    \n",
    "    ## COMPUTING NUMBER OF DAYS REPRESENTED BY EACH TD ##\n",
    "    sorted_TD = TD_of_days.groupby('TD_of_days').count()\n",
    "    sorted_TD.rename(columns={'day':'#days'}, inplace=True)\n",
    "    sorted_TD.reset_index(inplace=True)\n",
    "    sorted_TD.set_index(np.arange(1,Nbr_TD+1), inplace=True) # adding number of TD as index\n",
    "    \n",
    "    ## BUILDING T_H_TD MATRICE ##\n",
    "    # generate T_H_TD\n",
    "    TD_and_hour_array = np.ones((24*365,2))\n",
    "    for i in range(365):\n",
    "        TD_and_hour_array[i*24:(i+1)*24,0] = np.arange(1,25,1)\n",
    "        TD_and_hour_array[i*24:(i+1)*24,1] = TD_and_hour_array[i*24:(i+1)*24,1]*sorted_TD[sorted_TD['TD_of_days']==TD_of_days.loc[i,'TD_of_days']].index.values\n",
    "    T_H_TD = pd.DataFrame(TD_and_hour_array, index = np.arange(1,8761,1), columns=['H_of_D','TD_of_day'])\n",
    "    T_H_TD = T_H_TD.astype('int64')\n",
    "    # giving the right syntax\n",
    "    T_H_TD.reset_index(inplace=True)\n",
    "    T_H_TD.rename(columns={'index':'H_of_Y'}, inplace=True)\n",
    "    T_H_TD['par_g'] = '('\n",
    "    T_H_TD['par_d'] = ')'\n",
    "    T_H_TD['comma1'] = ','\n",
    "    T_H_TD['comma2'] = ','\n",
    "    # giving the right order to the columns\n",
    "    T_H_TD = T_H_TD[['par_g','H_of_Y','comma1','H_of_D','comma2','TD_of_day','par_d']]\n",
    "    \n",
    "    ## READING THE TIMESERIES IN DATA FILE ##\n",
    "    N_c = len(data) #number of countries\n",
    "    # READ FIRST COUNTRY #\n",
    "    #reading the timeseries\n",
    "    timeseries =  pd.read_excel(data[0],  sheet_name='1.1 Time Series',  header=[1], index_col=0, nrows = 8760 )\n",
    "    timeseries.drop(labels='period_duration [h]', axis=1, inplace=True)\n",
    "    ts_names = list(timeseries.columns) # names of the columns\n",
    "    timeseries.columns = [str(col) + '_' + countries[0] for col in timeseries.columns]\n",
    "\n",
    "    # READING THE OTHER COUNTRIES #\n",
    "    for i in np.arange(1,N_c):\n",
    "        #reading the timeseries\n",
    "        ts2 = pd.read_excel(data[i],  sheet_name='1.1 Time Series',  header=[1], index_col=0, nrows = 8760).drop(labels='period_duration [h]', axis=1)\n",
    "        ts2.columns = [str(col) + '_' + countries[i] for col in ts2.columns]\n",
    "        timeseries = timeseries.merge(ts2, left_index=True, right_index=True)\n",
    "\n",
    "    # COMPUTING THE NORM OVER THE YEAR ##\n",
    "    norm = timeseries.sum(axis=0)\n",
    "    norm.index.rename('Category', inplace=True)\n",
    "    norm.name = 'Norm'\n",
    "    \n",
    "    ## BUILDING TD TIMESERIES ##\n",
    "    # creating df with 2 columns : day of the year | hour in the day\n",
    "    day_and_hour_array = np.ones((24*365,2))\n",
    "    for i in range(365):\n",
    "        day_and_hour_array[i*24:(i+1)*24,0] = day_and_hour_array[i*24:(i+1)*24,0]*(i+1)\n",
    "        day_and_hour_array[i*24:(i+1)*24,1] = np.arange(1,25,1)\n",
    "    day_and_hour = pd.DataFrame(day_and_hour_array, index = np.arange(1,8761,1), columns=['D_of_H','H_of_D'])\n",
    "    day_and_hour = day_and_hour.astype('int64')\n",
    "    timeseries = timeseries.merge(day_and_hour,left_index=True, right_index=True)\n",
    "\n",
    "    #selecting timeseries of TD only\n",
    "    TD_ts = timeseries[timeseries['D_of_H'].isin(sorted_TD['TD_of_days'])]\n",
    "    \n",
    "    ## COMPUTING THE NORM_TD OVER THE YEAR FOR CORRECTION ##\n",
    "    # computing the sum of ts over each TD\n",
    "    agg_TD_ts = TD_ts.groupby('D_of_H').sum()\n",
    "    agg_TD_ts.reset_index(inplace=True)\n",
    "    agg_TD_ts.set_index(np.arange(1,13), inplace=True)\n",
    "    agg_TD_ts.drop(columns=['D_of_H','H_of_D'], inplace=True)\n",
    "    # multiplicating each TD by the number of day it represents\n",
    "    for c in agg_TD_ts.columns:\n",
    "        agg_TD_ts[c] = agg_TD_ts[c]*sorted_TD['#days']\n",
    "    # sum of new ts over the whole year\n",
    "    norm_TD = agg_TD_ts.sum()\n",
    "    \n",
    "    ## BUILDING THE DF WITH THE TS OF EACH TD FOR EACH CATEGORY ##\n",
    "    # pivoting TD_ts to obtain a (24,Nbr_TD*Nbr_ts*N_c)\n",
    "    all_TD_ts = TD_ts.pivot(index='H_of_D', columns='D_of_H')\n",
    "    \n",
    "    ## PRINTING 'ES_TD_'+str(Nbr_TD)+'TD.dat' ##\n",
    "    out_path = 'STEP_2_Energy_Model\\ESTD_'+str(Nbr_TD)+'TD.dat'\n",
    "    # printing description of file\n",
    "    with open(out_path, mode='w',newline='') as TD_file:\n",
    "        TD_writer = csv.writer(TD_file, delimiter='\\t', quotechar=' ', quoting=csv.QUOTE_MINIMAL)\n",
    "\n",
    "        TD_writer.writerow(['# -------------------------------------------------------------------------------------------------------------------------\t'])\n",
    "        TD_writer.writerow(['#\tEnergyScope TD is an open-source energy model suitable for country scale analysis. It is a simplified representation of an urban or national energy system accounting for the energy flows'])\n",
    "        TD_writer.writerow(['#\twithin its boundaries. Based on a hourly resolution, it optimises the design and operation of the energy system while minimizing the cost of the system.'])\n",
    "        TD_writer.writerow(['#\t'])\n",
    "        TD_writer.writerow(['#\tCopyright (C) <2018-2019> <Ecole Polytechnique Fédérale de Lausanne (EPFL), Switzerland and Université catholique de Louvain (UCLouvain), Belgium>'])\n",
    "        TD_writer.writerow(['#\t'])\n",
    "        TD_writer.writerow(['#\t'])\n",
    "        TD_writer.writerow(['#\tLicensed under the Apache License, Version 2.0 (the \"License\");'])\n",
    "        TD_writer.writerow(['#\tyou may not use this file except in compliance with the License.'])\n",
    "        TD_writer.writerow(['#\tYou may obtain a copy of the License at'])\n",
    "        TD_writer.writerow(['#\t'])\n",
    "        TD_writer.writerow(['#\thttp://www.apache.org/licenses/LICENSE-2.0'])\n",
    "        TD_writer.writerow(['#\t'])\n",
    "        TD_writer.writerow(['#\tUnless required by applicable law or agreed to in writing, software'])\n",
    "        TD_writer.writerow(['#\tdistributed under the License is distributed on an \"AS IS\" BASIS,'])\n",
    "        TD_writer.writerow(['#\tWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.'])\n",
    "        TD_writer.writerow(['#\tSee the License for the specific language governing permissions and'])\n",
    "        TD_writer.writerow(['#\tlimitations under the License.'])\n",
    "        TD_writer.writerow(['#\t'])\n",
    "        TD_writer.writerow(['#\tDescription and complete License: see LICENSE file.'])\n",
    "        TD_writer.writerow(['# -------------------------------------------------------------------------------------------------------------------------\t'])\n",
    "        TD_writer.writerow(['\t'])\n",
    "        TD_writer.writerow(['# SETS depending on TD\t'])\n",
    "        TD_writer.writerow(['# --------------------------\t'])\n",
    "        TD_writer.writerow(['param peak_sh_factor\t:=\t'])\n",
    "        peak_sh_factor = 1\n",
    "        for c in countries:\n",
    "            max_sh_TD = TD_ts.loc[:,'Space Heating (%_sh)_'+c].max()\n",
    "            max_sh_all = timeseries.loc[:,'Space Heating (%_sh)_'+c].max()\n",
    "            peak_sh_factor = max_sh_all/max_sh_TD\n",
    "            TD_writer.writerow([c + ' ' + str(peak_sh_factor)])\n",
    "        TD_writer.writerow([';\t\t'])\n",
    "        TD_writer.writerow(['\t\t'])\n",
    "        TD_writer.writerow(['#SETS [Figure 3]\t\t'])\n",
    "        TD_writer.writerow(['set T_H_TD := \t\t'])\n",
    "    \n",
    "    # printing T_H_TD param\n",
    "    T_H_TD.to_csv(out_path,sep='\\t', header=False, index=False, mode='a', quoting=csv.QUOTE_NONE)\n",
    "    \n",
    "    # printing interlude\n",
    "    with open(out_path, mode='a',newline='') as TD_file:\n",
    "        TD_writer = csv.writer(TD_file, delimiter='\\t', quotechar=' ', quoting=csv.QUOTE_MINIMAL)\n",
    "\n",
    "        TD_writer.writerow([';'])\n",
    "        TD_writer.writerow([''])\n",
    "        TD_writer.writerow(['# -----------------------------'])\n",
    "        TD_writer.writerow(['# PARAMETERS DEPENDING ON NUMBER OF TYPICAL DAYS : '])\n",
    "        TD_writer.writerow(['# -----------------------------'])\n",
    "        TD_writer.writerow([''])\n",
    "        \n",
    "    #if only 1 country\n",
    "    if N_c==1:\n",
    "        # printing EUD timeseries param\n",
    "        for l in EUD_params.keys():\n",
    "            with open(out_path, mode='a',newline='') as TD_file:\n",
    "                TD_writer = csv.writer(TD_file, delimiter='\\t', quotechar=' ', quoting=csv.QUOTE_MINIMAL)\n",
    "                TD_writer.writerow([EUD_params[l][0:-1]])    \n",
    "            for c in countries:\n",
    "                name = l+'_'+c\n",
    "                ts = all_TD_ts[name]\n",
    "                ts.columns = np.arange(1,13) \n",
    "                ts = ts*norm[name]/norm_TD[name]\n",
    "                ts.fillna(0, inplace=True)\n",
    "                ts.rename(columns = {ts.shape[1]:str(ts.shape[1])+' '+':='}, inplace=True)\n",
    "                ts.to_csv(out_path,sep='\\t', header=True, index=True, index_label='', mode='a', quoting=csv.QUOTE_NONE)\n",
    "            with open(out_path, mode='a',newline='') as TD_file:\n",
    "                TD_writer = csv.writer(TD_file, delimiter='\\t', quotechar=' ', quoting=csv.QUOTE_MINIMAL)\n",
    "                TD_writer.writerow(';')\n",
    "                TD_writer.writerow([''])\n",
    "\n",
    "        # printing c_p_t param #\n",
    "        with open(out_path, mode='a',newline='') as TD_file:\n",
    "            TD_writer = csv.writer(TD_file, delimiter='\\t', quotechar=' ', quoting=csv.QUOTE_MINIMAL)\n",
    "            TD_writer.writerow(['param c_p_t:='])    \n",
    "        # printing c_p_t part where 1 ts => 1 tech\n",
    "        for l in RES_params.keys():\n",
    "            for c in countries:\n",
    "                name = l+'_'+c\n",
    "                ts = all_TD_ts[name]\n",
    "                ts.columns = np.arange(1,13) \n",
    "                ts = ts*norm[name]/norm_TD[name]\n",
    "                ts.fillna(0, inplace=True)\n",
    "                ts.rename(columns = {ts.shape[1]:str(ts.shape[1])+' '+':='}, inplace=True)\n",
    "                ts.to_csv(out_path,sep='\\t', header=True, index=True, index_label='[\"' + RES_params[l] + '\",*,*] :', mode='a', quoting=csv.QUOTE_NONE)\n",
    "        # printing c_p_t part where 1 ts => more then 1 tech        \n",
    "        for l in RES_mult_params.keys():\n",
    "            for j in RES_mult_params[l]:\n",
    "                for c in countries:\n",
    "                    name = l+'_'+c\n",
    "                    ts = all_TD_ts[name]\n",
    "                    ts.columns = np.arange(1,13) \n",
    "                    ts = ts*norm[name]/norm_TD[name]\n",
    "                    ts.fillna(0, inplace=True)\n",
    "                    ts.rename(columns = {ts.shape[1]:str(ts.shape[1])+' '+':='}, inplace=True)\n",
    "                    ts.to_csv(out_path,sep='\\t', header=True, index=True, index_label='[\"' + j + '\",*,*] :', mode='a', quoting=csv.QUOTE_NONE)\n",
    "\n",
    "        with open(out_path, mode='a',newline='') as TD_file:\n",
    "            TD_writer = csv.writer(TD_file, delimiter='\\t', quotechar=' ', quoting=csv.QUOTE_MINIMAL)\n",
    "            TD_writer.writerow([';'])\n",
    "    else:\n",
    "        # printing EUD timeseries param\n",
    "        for l in EUD_params.keys():\n",
    "            with open(out_path, mode='a',newline='') as TD_file:\n",
    "                TD_writer = csv.writer(TD_file, delimiter='\\t', quotechar=' ', quoting=csv.QUOTE_MINIMAL)\n",
    "                TD_writer.writerow([EUD_params[l]])    \n",
    "            for c in countries:\n",
    "                name = l+'_'+c\n",
    "                ts = all_TD_ts[name]\n",
    "                ts.columns = np.arange(1,13) \n",
    "                ts = ts*norm[name]/norm_TD[name]\n",
    "                ts.fillna(0, inplace=True)\n",
    "                ts.rename(columns = {ts.shape[1]:str(ts.shape[1])+' '+':='}, inplace=True)\n",
    "                ts.to_csv(out_path,sep='\\t', header=True, index=True, index_label='[\"' + c + '\",*,*] :', mode='a', quoting=csv.QUOTE_NONE)\n",
    "            with open(out_path, mode='a',newline='') as TD_file:\n",
    "                TD_writer = csv.writer(TD_file, delimiter='\\t', quotechar=' ', quoting=csv.QUOTE_MINIMAL)\n",
    "                TD_writer.writerow(';')\n",
    "                TD_writer.writerow([''])\n",
    "\n",
    "        # printing c_p_t param #\n",
    "        with open(out_path, mode='a',newline='') as TD_file:\n",
    "            TD_writer = csv.writer(TD_file, delimiter='\\t', quotechar=' ', quoting=csv.QUOTE_MINIMAL)\n",
    "            TD_writer.writerow(['param c_p_t:='])    \n",
    "        # printing c_p_t part where 1 ts => 1 tech\n",
    "        for l in RES_params.keys():\n",
    "            for c in countries:\n",
    "                name = l+'_'+c\n",
    "                ts = all_TD_ts[name]\n",
    "                ts.columns = np.arange(1,13) \n",
    "                ts = ts*norm[name]/norm_TD[name]\n",
    "                ts.fillna(0, inplace=True)\n",
    "                ts.rename(columns = {ts.shape[1]:str(ts.shape[1])+' '+':='}, inplace=True)\n",
    "                ts.to_csv(out_path,sep='\\t', header=True, index=True, index_label='[\"' + RES_params[l] + '\",\"' + c + '\",*,*] :', mode='a', quoting=csv.QUOTE_NONE)\n",
    "        # printing c_p_t part where 1 ts => more then 1 tech        \n",
    "        for l in RES_mult_params.keys():\n",
    "            for j in RES_mult_params[l]:\n",
    "                for c in countries:\n",
    "                    name = l+'_'+c\n",
    "                    ts = all_TD_ts[name]\n",
    "                    ts.columns = np.arange(1,13) \n",
    "                    ts = ts*norm[name]/norm_TD[name]\n",
    "                    ts.fillna(0, inplace=True)\n",
    "                    ts.rename(columns = {ts.shape[1]:str(ts.shape[1])+' '+':='}, inplace=True)\n",
    "                    ts.to_csv(out_path,sep='\\t', header=True, index=True, index_label='[\"' + j + '\",\"' + c + '\",*,*] :', mode='a', quoting=csv.QUOTE_NONE)\n",
    "\n",
    "        with open(out_path, mode='a',newline='') as TD_file:\n",
    "            TD_writer = csv.writer(TD_file, delimiter='\\t', quotechar=' ', quoting=csv.QUOTE_MINIMAL)\n",
    "            TD_writer.writerow([';'])\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Calling the functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From DATA to STEP_1_IN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['Space Cooling_FR'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_270202/678640838.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mstep1_in\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcountries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_ts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNbr_TD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"--- %s seconds ---\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_270202/837753805.py\u001b[0m in \u001b[0;36mstep1_in\u001b[0;34m(countries, data, N_ts, Nbr_TD)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mweights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Category'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;31m# select columns of ts that matters for STEP1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0mweight_ts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnorm_ts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mts_names_all\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m     \u001b[0;31m# multiply each timeserie by its weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0mweight_ts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweight_ts\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/progs/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3509\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3510\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3511\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_indexer_strict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"columns\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3513\u001b[0m         \u001b[0;31m# take() does not accept boolean indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/progs/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   5794\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5795\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5796\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_if_missing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5797\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5798\u001b[0m         \u001b[0mkeyarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/progs/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   5857\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5858\u001b[0m             \u001b[0mnot_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmissing_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5859\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{not_found} not in index\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5860\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5861\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0moverload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['Space Cooling_FR'] not in index\""
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "step1_in(countries, data, N_ts, Nbr_TD)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From STEP_1_OUT to STEP_2_IN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 4.023488521575928 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "step2_in(countries, data, step1_out, EUD_params, RES_params, RES_mult_params, N_ts, Nbr_TD)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
